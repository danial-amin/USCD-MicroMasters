{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danial-amin/USCD-MicroMasters/blob/main/ML%20Fundamentals/Programming%20Assignments/10-word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4KtEXP28PJw"
      },
      "source": [
        "# Experiments with word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au5oTEtX8PJ0"
      },
      "source": [
        "In this notebook, we'll have some fun with **<font color=\"magenta\">word embeddings</font>**: distributed representations of words. \n",
        "\n",
        "We'll see how such an embedding can be constructed by applying principal component analysis to a suitably transformed matrix of word co-occurrence probabilities. For computational reasons, we'll use the moderately sized **Brown corpus of present-day American English** for this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cJ_oDdx8PJ1"
      },
      "source": [
        "## 1. Accessing the Brown corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7imnDNgQ8PJ1"
      },
      "source": [
        "The *Brown corpus* is available as part of the Python Natural Language Toolkit (`nltk`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMlDH21u8PJ2",
        "outputId": "386f60e7-7168-42c4-c178-d4c00f4f17c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import brown, stopwords\n",
        "from scipy.cluster.vq import kmeans2\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WtmUmMB8PJ4"
      },
      "source": [
        "The corpus consists of 500 samples of text drawn from a wide range of sources. When these are concatenated, they form a very long stream of over a million words, which is available as `brown.words()`. Let's look at the first 50 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epHKJ-AN8PJ4",
        "outputId": "e3e78e1b-6a1c-4038-e357-e1c4b4864efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "Fulton\n",
            "County\n",
            "Grand\n",
            "Jury\n",
            "said\n",
            "Friday\n",
            "an\n",
            "investigation\n",
            "of\n",
            "Atlanta's\n",
            "recent\n",
            "primary\n",
            "election\n",
            "produced\n",
            "``\n",
            "no\n",
            "evidence\n",
            "''\n",
            "that\n",
            "any\n",
            "irregularities\n",
            "took\n",
            "place\n",
            ".\n",
            "The\n",
            "jury\n",
            "further\n",
            "said\n",
            "in\n",
            "term-end\n",
            "presentments\n",
            "that\n",
            "the\n",
            "City\n",
            "Executive\n",
            "Committee\n",
            ",\n",
            "which\n",
            "had\n",
            "over-all\n",
            "charge\n",
            "of\n",
            "the\n",
            "election\n",
            ",\n",
            "``\n",
            "deserves\n",
            "the\n",
            "praise\n"
          ]
        }
      ],
      "source": [
        "for i in range(50):\n",
        "    print (brown.words()[i],)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213xHbd78PJ5"
      },
      "source": [
        "Before doing anything else, let's remove stopwords and punctuation and make everything lowercase. The resulting sequence will be stored in `my_word_stream`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F-gDq9vG8PJ6"
      },
      "outputs": [],
      "source": [
        "my_stopwords = set(stopwords.words('english'))\n",
        "word_stream = [str(w).lower() for w in brown.words() if w.lower() not in my_stopwords]\n",
        "my_word_stream = [w for w in word_stream if (len(w) > 1 and w.isalnum())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy0g_CFJ8PJ7"
      },
      "source": [
        "Here are the initial 20 words in `my_word_stream`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i938t9EI8PJ7",
        "outputId": "99198db6-37e9-4602-9627-59cbc0fb38e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['fulton',\n",
              " 'county',\n",
              " 'grand',\n",
              " 'jury',\n",
              " 'said',\n",
              " 'friday',\n",
              " 'investigation',\n",
              " 'recent',\n",
              " 'primary',\n",
              " 'election',\n",
              " 'produced',\n",
              " 'evidence',\n",
              " 'irregularities',\n",
              " 'took',\n",
              " 'place',\n",
              " 'jury',\n",
              " 'said',\n",
              " 'presentments',\n",
              " 'city',\n",
              " 'executive']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "my_word_stream[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlVuF3TW8PJ8"
      },
      "source": [
        "## 2. Computing co-occurrence probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9NwJJJV8PJ8"
      },
      "source": [
        "<font color=\"magenta\">Step 1: Get a list of words and their frequencies.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KU9LAW7q8PJ8"
      },
      "outputs": [],
      "source": [
        "N = len(my_word_stream)\n",
        "words = []\n",
        "totals = {}\n",
        "for i in range(1, N-1):\n",
        "    w = my_word_stream[i]\n",
        "    if w not in words:\n",
        "        words.append(w)\n",
        "        totals[w] = 0\n",
        "    totals[w] = totals[w] + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DKp9TQ28PJ9"
      },
      "source": [
        "<font color=\"magenta\">Step 2: Decide on the vocabulary.</font> There are two potentially distinct vocabularies: the words for which we will obtain embeddings (`vocab_words`) and the words we will consider when looking at context information (`context_words`). We will take the former to be all words that occur at least 20 times, and the latter to be all words that occur at least 100 times. These choices are pretty arbitrary: by all means, play around with them and find something better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "uU5Xh5qS8PJ9"
      },
      "outputs": [],
      "source": [
        "vocab_words = [w for w in words if totals[w] > 19]\n",
        "context_words = [w for w in words if totals[w] > 99]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_kmZYuQ8PJ9"
      },
      "source": [
        "How large are these two word lists? **Note down these numbers: you will need to enter them as part of this week's assignment.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOyMssCP8PJ-",
        "outputId": "7781282d-bf97-41d7-d6e8-2805c1010ff6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4720, 918)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(vocab_words), len(context_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcjAoeCd8PJ-"
      },
      "source": [
        "<font color=\"magenta\">Step 3: Get co-occurrence counts.</font> These are defined as follows, for a small constant `window_size=2`.\n",
        "\n",
        "* Let `w0` be any word in `vocab_words` and `w` any word in `context_words`.\n",
        "* Each time `w0` occurs in the corpus, look at the window of `window_size` words before and after it. If `w` appears in this window, we say it appears in the context of (this particular occurrence of) `w0`.\n",
        "* Define `counts[w0][w]` as the total number of times `w` occurs in the context of `w0`.\n",
        "\n",
        "The function `get_counts` computes the `counts` array, and returns it as a dictionary (of dictionaries)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LlRZnFTd8PJ-"
      },
      "outputs": [],
      "source": [
        "def get_counts(window_size=2):\n",
        "    counts = {}\n",
        "    for w0 in vocab_words:\n",
        "        counts[w0] = {}\n",
        "    for i in range(window_size, N-window_size):\n",
        "        w0 = my_word_stream[i]\n",
        "        if w0 in vocab_words:\n",
        "            for j in (list(range(-window_size,0)) + list(range(1,window_size+1))):\n",
        "                w = my_word_stream[i+j]\n",
        "                if w in context_words:\n",
        "                    if w not in counts[w0].keys():\n",
        "                        counts[w0][w] = 1\n",
        "                    else:\n",
        "                        counts[w0][w] = counts[w0][w] + 1\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMUpKB4-8PJ_"
      },
      "source": [
        "Define `probs[w0][]` to be the distribution over the context of `w0`, that is:\n",
        "* `probs[w0][w] = counts[w0][w] / (sum of all counts[w0][])`\n",
        "\n",
        "This is computed by the function `get_co_occurrence_dictionary`, given `counts`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aLXMJs538PJ_"
      },
      "outputs": [],
      "source": [
        "def get_co_occurrence_dictionary(counts):\n",
        "    probs = {}\n",
        "    for w0 in counts.keys():\n",
        "        sum = 0\n",
        "        for w in counts[w0].keys():\n",
        "            sum = sum + counts[w0][w]\n",
        "        if sum > 0:\n",
        "            probs[w0] = {}\n",
        "            for w in counts[w0].keys():\n",
        "                probs[w0][w] = float(counts[w0][w])/float(sum)\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MRTI6oq8PJ_"
      },
      "source": [
        "The final piece of information we need is the frequency of different context words. The function below, `get_context_word_distribution`, takes `counts` as input and returns (again, in dictionary form) the array:\n",
        "\n",
        "* `context_frequency[w]` = sum of all `counts[][w]` / sum of all `counts[][]` "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FRDWQjLU8PJ_"
      },
      "outputs": [],
      "source": [
        "def get_context_word_distribution(counts):\n",
        "    counts_context = {}\n",
        "    sum_context = 0\n",
        "    context_frequency = {}\n",
        "    for w in context_words:\n",
        "        counts_context[w] = 0\n",
        "    for w0 in counts.keys():\n",
        "        for w in counts[w0].keys():\n",
        "            counts_context[w] = counts_context[w] + counts[w0][w]\n",
        "            sum_context = sum_context + counts[w0][w]\n",
        "    for w in context_words:\n",
        "        context_frequency[w] = float(counts_context[w])/float(sum_context)\n",
        "    return context_frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poWE9bPS8PKA"
      },
      "source": [
        "## 3. The embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXixLDhJ8PKA"
      },
      "source": [
        "Based on the various pieces of information above, we compute the **pointwise mutual information matrix**:\n",
        "* `PMI[i,j] = MAX(0, log probs[ith vocab word][jth context word] - log context_frequency[jth context word])`\n",
        "\n",
        "The embedding of any word can then be taken as the corresponding row of this matrix. However, to reduce the dimension, we will apply PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWntL4p88PKA",
        "outputId": "d7d2aee3-1067-48a6-d3f7-432367d85546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing counts and distributions\n",
            "Computing pointwise mutual information\n"
          ]
        }
      ],
      "source": [
        "print (\"Computing counts and distributions\")\n",
        "counts = get_counts(2)\n",
        "probs = get_co_occurrence_dictionary(counts)\n",
        "context_frequency = get_context_word_distribution(counts)\n",
        "#\n",
        "print (\"Computing pointwise mutual information\")\n",
        "n_vocab = len(vocab_words)\n",
        "n_context = len(context_words)\n",
        "pmi = np.zeros((n_vocab, n_context))\n",
        "for i in range(0, n_vocab):\n",
        "    w0 = vocab_words[i]\n",
        "    for w in probs[w0].keys():\n",
        "        j = context_words.index(w)\n",
        "        pmi[i,j] = max(0.0, np.log(probs[w0][w]) - np.log(context_frequency[w]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WTu74Qn8PKA"
      },
      "source": [
        "Now reduce the dimension of the PMI vectors using principal component analysis. Here we bring it down to 100 dimensions, and then normalize the vectors to unit length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gDvo5HJi8PKA"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=100)\n",
        "vecs = pca.fit_transform(pmi)\n",
        "for i in range(0,n_vocab):\n",
        "    vecs[i] = vecs[i]/np.linalg.norm(vecs[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqj9GVgG8PKB"
      },
      "source": [
        "It is useful to save this embedding so that it doesn't need to be computed every time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MXd2qfzF8PKB"
      },
      "outputs": [],
      "source": [
        "fd = open(\"embedding.pickle\", \"wb\")\n",
        "pickle.dump(vocab_words, fd)\n",
        "pickle.dump(context_words, fd)\n",
        "pickle.dump(vecs, fd)\n",
        "fd.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X0faqr58PKB"
      },
      "source": [
        "## 4. Experimenting with the embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Fxqtr18PKB"
      },
      "source": [
        "We can get some insight into the embedding by looking at the nearest neighbor of different words in the embedded space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BxtF3a1H8PKB"
      },
      "outputs": [],
      "source": [
        "def word_NN(w):\n",
        "    if not(w in vocab_words):\n",
        "        print (\"Unknown word\")\n",
        "        return\n",
        "    v = vecs[vocab_words.index(w)]\n",
        "    neighbor = 0\n",
        "    curr_dist = np.linalg.norm(v - vecs[0])\n",
        "    for i in range(1, n_vocab):\n",
        "        dist = np.linalg.norm(v - vecs[i])\n",
        "        if (dist < curr_dist) and (dist > 0.0):\n",
        "            neighbor = i\n",
        "            curr_dist = dist\n",
        "    return vocab_words[neighbor]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BQ9NenI8PKC"
      },
      "source": [
        "**Note down the answers to the following queries. You will need to enter them as part of this week's assignment.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lvOXNL_j8PKC",
        "outputId": "feed29e2-69e0-47f8-871a-da757822d2f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'artery'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "word_NN('pulmonary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bHi1I94N8PKC",
        "outputId": "be9a6448-3ba4-43d2-8835-bb89c946d6cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'world'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "word_NN('communism')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1gsXU2RU8PKC",
        "outputId": "09fa7d75-437a-47e3-9189-25bbd103df52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "word_NN('world')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n6JnRGPB8PKC",
        "outputId": "e1d26cb6-1375-4974-cf54-a2bbb1914c22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rome'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "word_NN('london')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Kpcr5N_58PKC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "navigate_num": "#000000",
        "navigate_text": "#333333",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700",
        "sidebar_border": "#EEEEEE",
        "wrapper_background": "#FFFFFF"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "12px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": false,
      "sideBar": true,
      "threshold": 4,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false,
      "widenNotebook": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}